{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119d1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "import polars as pl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False \n",
    "LGB = True\n",
    "NN = False\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07e81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9436017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c0dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70f8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74aaddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    \n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    for window in [3,5,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    df = imbalance_features(df)\n",
    "    gc.collect() \n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f497b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e6cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "    \n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd017bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9839acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 160\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.0694\n",
      "[200]\tvalid_0's l1: 6.0263\n",
      "[300]\tvalid_0's l1: 6.00328\n",
      "[400]\tvalid_0's l1: 5.98357\n",
      "[500]\tvalid_0's l1: 5.96537\n",
      "[600]\tvalid_0's l1: 5.94998\n",
      "[700]\tvalid_0's l1: 5.9353\n",
      "[800]\tvalid_0's l1: 5.92203\n",
      "[900]\tvalid_0's l1: 5.9099\n",
      "[1000]\tvalid_0's l1: 5.89834\n",
      "[1100]\tvalid_0's l1: 5.88826\n",
      "[1200]\tvalid_0's l1: 5.87961\n",
      "[1300]\tvalid_0's l1: 5.87065\n",
      "[1400]\tvalid_0's l1: 5.86218\n",
      "[1500]\tvalid_0's l1: 5.85368\n",
      "[1600]\tvalid_0's l1: 5.84538\n",
      "[1700]\tvalid_0's l1: 5.83815\n",
      "[1800]\tvalid_0's l1: 5.83109\n",
      "[1900]\tvalid_0's l1: 5.82366\n",
      "[2000]\tvalid_0's l1: 5.81601\n",
      "[2100]\tvalid_0's l1: 5.80865\n",
      "[2200]\tvalid_0's l1: 5.80133\n",
      "[2300]\tvalid_0's l1: 5.79441\n",
      "[2400]\tvalid_0's l1: 5.7872\n",
      "[2500]\tvalid_0's l1: 5.78002\n",
      "[2600]\tvalid_0's l1: 5.77298\n",
      "[2700]\tvalid_0's l1: 5.76654\n",
      "[2800]\tvalid_0's l1: 5.76074\n",
      "[2900]\tvalid_0's l1: 5.75416\n",
      "[3000]\tvalid_0's l1: 5.74728\n",
      "[3100]\tvalid_0's l1: 5.7412\n",
      "[3200]\tvalid_0's l1: 5.73413\n",
      "[3300]\tvalid_0's l1: 5.72758\n",
      "[3400]\tvalid_0's l1: 5.72162\n",
      "[3500]\tvalid_0's l1: 5.71521\n",
      "[3600]\tvalid_0's l1: 5.70945\n",
      "[3700]\tvalid_0's l1: 5.70327\n",
      "[3800]\tvalid_0's l1: 5.69745\n",
      "[4200]\tvalid_0's l1: 5.67464\n",
      "[4300]\tvalid_0's l1: 5.66939\n",
      "[4400]\tvalid_0's l1: 5.66306\n",
      "[4500]\tvalid_0's l1: 5.65775\n",
      "[4600]\tvalid_0's l1: 5.65249\n",
      "[4700]\tvalid_0's l1: 5.6471\n",
      "[4800]\tvalid_0's l1: 5.64125\n",
      "[4900]\tvalid_0's l1: 5.63609\n",
      "[5000]\tvalid_0's l1: 5.63127\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 5.63127\n",
      "Model for fold 1 saved to modelitos_para_despues/doblez_1.txt\n",
      ":LGB Fold 1 MAE: 5.631266002763471\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.30898\n",
      "[200]\tvalid_0's l1: 7.25443\n",
      "[300]\tvalid_0's l1: 7.22579\n",
      "[400]\tvalid_0's l1: 7.2028\n",
      "[500]\tvalid_0's l1: 7.18189\n",
      "[600]\tvalid_0's l1: 7.16331\n",
      "[700]\tvalid_0's l1: 7.14629\n",
      "[800]\tvalid_0's l1: 7.13002\n",
      "[900]\tvalid_0's l1: 7.11584\n",
      "[1000]\tvalid_0's l1: 7.10264\n",
      "[1100]\tvalid_0's l1: 7.0912\n",
      "[1200]\tvalid_0's l1: 7.07998\n",
      "[1300]\tvalid_0's l1: 7.06796\n",
      "[1400]\tvalid_0's l1: 7.05787\n",
      "[1500]\tvalid_0's l1: 7.04728\n",
      "[1600]\tvalid_0's l1: 7.03764\n",
      "[1700]\tvalid_0's l1: 7.02761\n",
      "[1800]\tvalid_0's l1: 7.01825\n",
      "[1900]\tvalid_0's l1: 7.00921\n",
      "[2400]\tvalid_0's l1: 6.96726\n",
      "[2500]\tvalid_0's l1: 6.95968\n",
      "[2600]\tvalid_0's l1: 6.95081\n",
      "[2700]\tvalid_0's l1: 6.94197\n",
      "[2800]\tvalid_0's l1: 6.93324\n",
      "[2900]\tvalid_0's l1: 6.92523\n",
      "[3000]\tvalid_0's l1: 6.91726\n",
      "[3100]\tvalid_0's l1: 6.90996\n",
      "[3200]\tvalid_0's l1: 6.90154\n",
      "[3300]\tvalid_0's l1: 6.89411\n",
      "[3400]\tvalid_0's l1: 6.88689\n",
      "[3500]\tvalid_0's l1: 6.87819\n",
      "[3600]\tvalid_0's l1: 6.87099\n",
      "[3700]\tvalid_0's l1: 6.86365\n",
      "[3800]\tvalid_0's l1: 6.85643\n",
      "[3900]\tvalid_0's l1: 6.84964\n",
      "[4000]\tvalid_0's l1: 6.84184\n",
      "[4100]\tvalid_0's l1: 6.83438\n",
      "[4200]\tvalid_0's l1: 6.82718\n",
      "[4300]\tvalid_0's l1: 6.82048\n",
      "[4400]\tvalid_0's l1: 6.81408\n",
      "[4500]\tvalid_0's l1: 6.80706\n",
      "[4600]\tvalid_0's l1: 6.80079\n",
      "[4700]\tvalid_0's l1: 6.79418\n",
      "[4800]\tvalid_0's l1: 6.787\n",
      "[4900]\tvalid_0's l1: 6.77974\n",
      "[5000]\tvalid_0's l1: 6.77399\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 6.77399\n",
      "Model for fold 2 saved to modelitos_para_despues/doblez_2.txt\n",
      ":LGB Fold 2 MAE: 6.773986311046863\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.87765\n",
      "[200]\tvalid_0's l1: 6.82997\n",
      "[300]\tvalid_0's l1: 6.80232\n",
      "[400]\tvalid_0's l1: 6.77878\n",
      "[500]\tvalid_0's l1: 6.75759\n",
      "[600]\tvalid_0's l1: 6.7384\n",
      "[700]\tvalid_0's l1: 6.72099\n",
      "[800]\tvalid_0's l1: 6.70514\n",
      "[900]\tvalid_0's l1: 6.6913\n",
      "[1000]\tvalid_0's l1: 6.67876\n",
      "[1100]\tvalid_0's l1: 6.666\n",
      "[1200]\tvalid_0's l1: 6.65505\n",
      "[1300]\tvalid_0's l1: 6.64483\n",
      "[1400]\tvalid_0's l1: 6.63536\n",
      "[1500]\tvalid_0's l1: 6.62546\n",
      "[1600]\tvalid_0's l1: 6.6163\n",
      "[1700]\tvalid_0's l1: 6.60739\n",
      "[1800]\tvalid_0's l1: 6.59825\n",
      "[1900]\tvalid_0's l1: 6.58868\n",
      "[2000]\tvalid_0's l1: 6.57989\n",
      "[2100]\tvalid_0's l1: 6.57106\n",
      "[2200]\tvalid_0's l1: 6.56192\n",
      "[2300]\tvalid_0's l1: 6.55313\n",
      "[2400]\tvalid_0's l1: 6.54528\n",
      "[2500]\tvalid_0's l1: 6.53632\n",
      "[2600]\tvalid_0's l1: 6.52803\n",
      "[2700]\tvalid_0's l1: 6.51782\n",
      "[2800]\tvalid_0's l1: 6.51057\n",
      "[2900]\tvalid_0's l1: 6.50353\n",
      "[3000]\tvalid_0's l1: 6.49617\n",
      "[3100]\tvalid_0's l1: 6.48875\n",
      "[3200]\tvalid_0's l1: 6.48147\n",
      "[3300]\tvalid_0's l1: 6.47386\n",
      "[3400]\tvalid_0's l1: 6.46622\n",
      "[3500]\tvalid_0's l1: 6.45833\n",
      "[3600]\tvalid_0's l1: 6.45191\n",
      "[3700]\tvalid_0's l1: 6.44503\n",
      "[3800]\tvalid_0's l1: 6.4377\n",
      "[3900]\tvalid_0's l1: 6.43068\n",
      "[4000]\tvalid_0's l1: 6.42436\n",
      "[4100]\tvalid_0's l1: 6.41796\n",
      "[4200]\tvalid_0's l1: 6.41183\n",
      "[4300]\tvalid_0's l1: 6.4048\n",
      "[4400]\tvalid_0's l1: 6.39746\n",
      "[4500]\tvalid_0's l1: 6.39091\n",
      "[4600]\tvalid_0's l1: 6.38479\n",
      "[4700]\tvalid_0's l1: 6.37759\n",
      "[4800]\tvalid_0's l1: 6.37121\n",
      "[4900]\tvalid_0's l1: 6.36501\n",
      "[5000]\tvalid_0's l1: 6.35885\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 6.35885\n",
      "Model for fold 3 saved to modelitos_para_despues/doblez_3.txt\n",
      ":LGB Fold 3 MAE: 6.3588533099219084\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.92031\n",
      "[200]\tvalid_0's l1: 6.8769\n",
      "[300]\tvalid_0's l1: 6.85015\n",
      "[400]\tvalid_0's l1: 6.82602\n",
      "[500]\tvalid_0's l1: 6.80347\n",
      "[600]\tvalid_0's l1: 6.78302\n",
      "[700]\tvalid_0's l1: 6.7633\n",
      "[800]\tvalid_0's l1: 6.74544\n",
      "[900]\tvalid_0's l1: 6.72912\n",
      "[1000]\tvalid_0's l1: 6.71377\n",
      "[1100]\tvalid_0's l1: 6.69987\n",
      "[1200]\tvalid_0's l1: 6.68674\n",
      "[1300]\tvalid_0's l1: 6.67342\n",
      "[1400]\tvalid_0's l1: 6.66144\n",
      "[1500]\tvalid_0's l1: 6.64951\n",
      "[1600]\tvalid_0's l1: 6.63786\n",
      "[1700]\tvalid_0's l1: 6.62756\n",
      "[1800]\tvalid_0's l1: 6.61711\n",
      "[1900]\tvalid_0's l1: 6.60588\n",
      "[2000]\tvalid_0's l1: 6.59534\n",
      "[2100]\tvalid_0's l1: 6.58545\n",
      "[2200]\tvalid_0's l1: 6.57496\n",
      "[2300]\tvalid_0's l1: 6.56571\n",
      "[2400]\tvalid_0's l1: 6.55589\n",
      "[2500]\tvalid_0's l1: 6.54627\n",
      "[2600]\tvalid_0's l1: 6.53724\n",
      "[2700]\tvalid_0's l1: 6.52777\n",
      "[2800]\tvalid_0's l1: 6.51838\n",
      "[2900]\tvalid_0's l1: 6.51008\n",
      "[3000]\tvalid_0's l1: 6.50155\n",
      "[3100]\tvalid_0's l1: 6.49232\n",
      "[3200]\tvalid_0's l1: 6.48297\n",
      "[3300]\tvalid_0's l1: 6.47469\n",
      "[3400]\tvalid_0's l1: 6.46554\n",
      "[3500]\tvalid_0's l1: 6.45681\n",
      "[3600]\tvalid_0's l1: 6.44806\n",
      "[3700]\tvalid_0's l1: 6.43901\n",
      "[3800]\tvalid_0's l1: 6.43074\n",
      "[3900]\tvalid_0's l1: 6.42258\n",
      "[4000]\tvalid_0's l1: 6.41435\n",
      "[4100]\tvalid_0's l1: 6.40626\n",
      "[4200]\tvalid_0's l1: 6.39934\n",
      "[4300]\tvalid_0's l1: 6.39143\n",
      "[4400]\tvalid_0's l1: 6.38375\n",
      "[4500]\tvalid_0's l1: 6.37598\n",
      "[4600]\tvalid_0's l1: 6.36958\n",
      "[4700]\tvalid_0's l1: 6.36102\n",
      "[4800]\tvalid_0's l1: 6.35305\n",
      "[4900]\tvalid_0's l1: 6.3458\n",
      "[5000]\tvalid_0's l1: 6.3388\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 6.3388\n",
      "Model for fold 4 saved to modelitos_para_despues/doblez_4.txt\n",
      ":LGB Fold 4 MAE: 6.33880472799056\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.91129\n",
      "[200]\tvalid_0's l1: 5.87691\n",
      "[300]\tvalid_0's l1: 5.85275\n",
      "[400]\tvalid_0's l1: 5.83\n",
      "[500]\tvalid_0's l1: 5.80904\n",
      "[600]\tvalid_0's l1: 5.78965\n",
      "[700]\tvalid_0's l1: 5.7714\n",
      "[800]\tvalid_0's l1: 5.75645\n",
      "[900]\tvalid_0's l1: 5.74262\n",
      "[1000]\tvalid_0's l1: 5.72937\n",
      "[1100]\tvalid_0's l1: 5.71739\n",
      "[1200]\tvalid_0's l1: 5.70686\n",
      "[1300]\tvalid_0's l1: 5.69644\n",
      "[1400]\tvalid_0's l1: 5.686\n",
      "[1500]\tvalid_0's l1: 5.67624\n",
      "[1600]\tvalid_0's l1: 5.66593\n",
      "[1700]\tvalid_0's l1: 5.65674\n",
      "[1800]\tvalid_0's l1: 5.64648\n",
      "[1900]\tvalid_0's l1: 5.63656\n",
      "[2000]\tvalid_0's l1: 5.62759\n",
      "[2100]\tvalid_0's l1: 5.61907\n",
      "[2200]\tvalid_0's l1: 5.61002\n",
      "[2300]\tvalid_0's l1: 5.60187\n",
      "[2400]\tvalid_0's l1: 5.59348\n",
      "[2500]\tvalid_0's l1: 5.58489\n",
      "[2600]\tvalid_0's l1: 5.577\n",
      "[2700]\tvalid_0's l1: 5.56823\n",
      "[2800]\tvalid_0's l1: 5.55951\n",
      "[2900]\tvalid_0's l1: 5.55181\n",
      "[3000]\tvalid_0's l1: 5.54406\n",
      "[3100]\tvalid_0's l1: 5.53604\n",
      "[3200]\tvalid_0's l1: 5.52772\n",
      "[3300]\tvalid_0's l1: 5.5198\n",
      "[3400]\tvalid_0's l1: 5.51238\n",
      "[3500]\tvalid_0's l1: 5.50493\n",
      "[3600]\tvalid_0's l1: 5.49827\n",
      "[3700]\tvalid_0's l1: 5.49132\n",
      "[3800]\tvalid_0's l1: 5.48434\n",
      "[3900]\tvalid_0's l1: 5.47695\n",
      "[4000]\tvalid_0's l1: 5.46988\n",
      "[4100]\tvalid_0's l1: 5.46269\n",
      "[4200]\tvalid_0's l1: 5.4554\n",
      "[4300]\tvalid_0's l1: 5.44849\n",
      "[4400]\tvalid_0's l1: 5.44225\n",
      "[4500]\tvalid_0's l1: 5.43552\n",
      "[4600]\tvalid_0's l1: 5.42905\n",
      "[4700]\tvalid_0's l1: 5.42241\n",
      "[4800]\tvalid_0's l1: 5.41624\n",
      "[4900]\tvalid_0's l1: 5.4102\n",
      "[5000]\tvalid_0's l1: 5.40303\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 5.40303\n",
      "Model for fold 5 saved to modelitos_para_despues/doblez_5.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":LGB Fold 5 MAE: 5.40302533562934\n",
      "Fold 6 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.36792\n",
      "[200]\tvalid_0's l1: 6.32293\n",
      "[300]\tvalid_0's l1: 6.29143\n",
      "[400]\tvalid_0's l1: 6.26226\n",
      "[500]\tvalid_0's l1: 6.23625\n",
      "[600]\tvalid_0's l1: 6.21193\n",
      "[700]\tvalid_0's l1: 6.19006\n",
      "[800]\tvalid_0's l1: 6.1694\n",
      "[900]\tvalid_0's l1: 6.15125\n",
      "[1000]\tvalid_0's l1: 6.13491\n",
      "[1100]\tvalid_0's l1: 6.11917\n",
      "[1200]\tvalid_0's l1: 6.10384\n",
      "[1300]\tvalid_0's l1: 6.09009\n",
      "[1400]\tvalid_0's l1: 6.07662\n",
      "[1500]\tvalid_0's l1: 6.06243\n",
      "[1600]\tvalid_0's l1: 6.04975\n",
      "[1700]\tvalid_0's l1: 6.03754\n",
      "[1800]\tvalid_0's l1: 6.02503\n",
      "[1900]\tvalid_0's l1: 6.01273\n",
      "[2000]\tvalid_0's l1: 6.00041\n",
      "[2100]\tvalid_0's l1: 5.98835\n",
      "[2200]\tvalid_0's l1: 5.977\n",
      "[2300]\tvalid_0's l1: 5.96516\n",
      "[2400]\tvalid_0's l1: 5.95415\n",
      "[2500]\tvalid_0's l1: 5.94262\n",
      "[2600]\tvalid_0's l1: 5.93169\n",
      "[2700]\tvalid_0's l1: 5.92054\n",
      "[2800]\tvalid_0's l1: 5.91113\n",
      "[2900]\tvalid_0's l1: 5.90138\n",
      "[3000]\tvalid_0's l1: 5.89134\n",
      "[3100]\tvalid_0's l1: 5.88105\n",
      "[3200]\tvalid_0's l1: 5.87075\n",
      "[3300]\tvalid_0's l1: 5.86076\n",
      "[3400]\tvalid_0's l1: 5.85088\n",
      "[3500]\tvalid_0's l1: 5.84089\n",
      "[3600]\tvalid_0's l1: 5.83203\n",
      "[3700]\tvalid_0's l1: 5.82487\n",
      "[3800]\tvalid_0's l1: 5.81855\n",
      "[3900]\tvalid_0's l1: 5.81219\n",
      "[4000]\tvalid_0's l1: 5.80582\n",
      "[4100]\tvalid_0's l1: 5.7995\n",
      "[4200]\tvalid_0's l1: 5.79546\n",
      "[4300]\tvalid_0's l1: 5.79221\n",
      "[4400]\tvalid_0's l1: 5.78813\n",
      "[4500]\tvalid_0's l1: 5.78048\n",
      "[4600]\tvalid_0's l1: 5.77638\n",
      "[4700]\tvalid_0's l1: 5.77179\n",
      "[4800]\tvalid_0's l1: 5.76759\n",
      "[4900]\tvalid_0's l1: 5.76359\n",
      "[5000]\tvalid_0's l1: 5.76011\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 5.76011\n",
      "Model for fold 6 saved to modelitos_para_despues/doblez_6.txt\n",
      ":LGB Fold 6 MAE: 5.760110699037941\n",
      "Fold 7 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.27821\n",
      "[200]\tvalid_0's l1: 6.22961\n",
      "[300]\tvalid_0's l1: 6.19311\n",
      "[400]\tvalid_0's l1: 6.15841\n",
      "[500]\tvalid_0's l1: 6.12682\n",
      "[600]\tvalid_0's l1: 6.09732\n",
      "[700]\tvalid_0's l1: 6.0714\n",
      "[800]\tvalid_0's l1: 6.04736\n",
      "[900]\tvalid_0's l1: 6.02675\n",
      "[1000]\tvalid_0's l1: 6.00738\n",
      "[1100]\tvalid_0's l1: 5.98886\n",
      "[1200]\tvalid_0's l1: 5.97311\n",
      "[1300]\tvalid_0's l1: 5.95797\n",
      "[1400]\tvalid_0's l1: 5.94155\n",
      "[1500]\tvalid_0's l1: 5.92532\n",
      "[1600]\tvalid_0's l1: 5.90829\n",
      "[1700]\tvalid_0's l1: 5.89433\n",
      "[1800]\tvalid_0's l1: 5.87994\n",
      "[1900]\tvalid_0's l1: 5.86592\n",
      "[2000]\tvalid_0's l1: 5.85101\n",
      "[2100]\tvalid_0's l1: 5.83852\n",
      "[2200]\tvalid_0's l1: 5.82492\n",
      "[2300]\tvalid_0's l1: 5.81135\n",
      "[2400]\tvalid_0's l1: 5.79804\n",
      "[2500]\tvalid_0's l1: 5.78489\n",
      "[2600]\tvalid_0's l1: 5.77281\n",
      "[2700]\tvalid_0's l1: 5.75975\n",
      "[2800]\tvalid_0's l1: 5.74783\n",
      "[2900]\tvalid_0's l1: 5.73632\n",
      "[3000]\tvalid_0's l1: 5.7248\n",
      "[3100]\tvalid_0's l1: 5.71538\n",
      "[3200]\tvalid_0's l1: 5.70678\n",
      "[3300]\tvalid_0's l1: 5.69855\n",
      "[3400]\tvalid_0's l1: 5.69055\n",
      "[3500]\tvalid_0's l1: 5.6835\n",
      "[3600]\tvalid_0's l1: 5.67694\n",
      "[3700]\tvalid_0's l1: 5.67154\n",
      "[3800]\tvalid_0's l1: 5.66649\n",
      "[3900]\tvalid_0's l1: 5.66219\n",
      "[4000]\tvalid_0's l1: 5.65741\n",
      "[4100]\tvalid_0's l1: 5.65243\n",
      "[4200]\tvalid_0's l1: 5.64788\n",
      "[4300]\tvalid_0's l1: 5.6434\n",
      "[4400]\tvalid_0's l1: 5.63902\n",
      "[4500]\tvalid_0's l1: 5.63512\n",
      "[4600]\tvalid_0's l1: 5.63114\n",
      "[4700]\tvalid_0's l1: 5.62741\n",
      "[4800]\tvalid_0's l1: 5.62388\n",
      "[4900]\tvalid_0's l1: 5.62048\n",
      "[5000]\tvalid_0's l1: 5.61564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 5.61564\n",
      "Model for fold 7 saved to modelitos_para_despues/doblez_7.txt\n",
      ":LGB Fold 7 MAE: 5.615636379927347\n",
      "Fold 8 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.06319\n",
      "[200]\tvalid_0's l1: 6.00723\n",
      "[300]\tvalid_0's l1: 5.96349\n",
      "[400]\tvalid_0's l1: 5.92218\n",
      "[500]\tvalid_0's l1: 5.88505\n",
      "[600]\tvalid_0's l1: 5.85207\n",
      "[700]\tvalid_0's l1: 5.82108\n",
      "[800]\tvalid_0's l1: 5.7944\n",
      "[900]\tvalid_0's l1: 5.76994\n",
      "[1000]\tvalid_0's l1: 5.74687\n",
      "[1100]\tvalid_0's l1: 5.72535\n",
      "[1200]\tvalid_0's l1: 5.70441\n",
      "[1300]\tvalid_0's l1: 5.68466\n",
      "[1400]\tvalid_0's l1: 5.66618\n",
      "[1500]\tvalid_0's l1: 5.64751\n",
      "[1600]\tvalid_0's l1: 5.62911\n",
      "[1700]\tvalid_0's l1: 5.60991\n",
      "[1800]\tvalid_0's l1: 5.59246\n",
      "[1900]\tvalid_0's l1: 5.57632\n",
      "[2000]\tvalid_0's l1: 5.56065\n",
      "[2100]\tvalid_0's l1: 5.54501\n",
      "[2200]\tvalid_0's l1: 5.53025\n",
      "[2300]\tvalid_0's l1: 5.51498\n",
      "[2400]\tvalid_0's l1: 5.49995\n",
      "[2500]\tvalid_0's l1: 5.48527\n",
      "[2600]\tvalid_0's l1: 5.47108\n",
      "[2700]\tvalid_0's l1: 5.45726\n",
      "[2800]\tvalid_0's l1: 5.4436\n",
      "[2900]\tvalid_0's l1: 5.43069\n",
      "[3000]\tvalid_0's l1: 5.41678\n",
      "[3100]\tvalid_0's l1: 5.40279\n",
      "[3200]\tvalid_0's l1: 5.38976\n",
      "[3300]\tvalid_0's l1: 5.37658\n",
      "[3400]\tvalid_0's l1: 5.36424\n",
      "[3500]\tvalid_0's l1: 5.35145\n",
      "[3600]\tvalid_0's l1: 5.33922\n",
      "[3700]\tvalid_0's l1: 5.32683\n",
      "[3800]\tvalid_0's l1: 5.31501\n",
      "[3900]\tvalid_0's l1: 5.30348\n",
      "[4000]\tvalid_0's l1: 5.29378\n",
      "[4100]\tvalid_0's l1: 5.28476\n",
      "[4200]\tvalid_0's l1: 5.27742\n",
      "[4300]\tvalid_0's l1: 5.27138\n",
      "[4400]\tvalid_0's l1: 5.2647\n",
      "[4500]\tvalid_0's l1: 5.2579\n",
      "[4600]\tvalid_0's l1: 5.25224\n",
      "[4700]\tvalid_0's l1: 5.24709\n",
      "[4800]\tvalid_0's l1: 5.24236\n",
      "[4900]\tvalid_0's l1: 5.23717\n",
      "[5000]\tvalid_0's l1: 5.23205\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 5.23205\n",
      "Model for fold 8 saved to modelitos_para_despues/doblez_8.txt\n",
      ":LGB Fold 8 MAE: 5.232050808147341\n",
      "Fold 9 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.83845\n",
      "[200]\tvalid_0's l1: 5.77095\n",
      "[300]\tvalid_0's l1: 5.7123\n",
      "[400]\tvalid_0's l1: 5.65514\n",
      "[500]\tvalid_0's l1: 5.60461\n",
      "[600]\tvalid_0's l1: 5.5592\n",
      "[700]\tvalid_0's l1: 5.51931\n",
      "[800]\tvalid_0's l1: 5.48403\n",
      "[900]\tvalid_0's l1: 5.45048\n",
      "[1000]\tvalid_0's l1: 5.41955\n",
      "[1100]\tvalid_0's l1: 5.3905\n",
      "[1200]\tvalid_0's l1: 5.36102\n",
      "[1300]\tvalid_0's l1: 5.33346\n",
      "[1400]\tvalid_0's l1: 5.30789\n",
      "[1500]\tvalid_0's l1: 5.28572\n",
      "[1600]\tvalid_0's l1: 5.26224\n",
      "[1700]\tvalid_0's l1: 5.23922\n",
      "[1800]\tvalid_0's l1: 5.21677\n",
      "[1900]\tvalid_0's l1: 5.19624\n",
      "[2000]\tvalid_0's l1: 5.17418\n",
      "[2100]\tvalid_0's l1: 5.1541\n",
      "[2200]\tvalid_0's l1: 5.13345\n",
      "[2300]\tvalid_0's l1: 5.11482\n",
      "[2400]\tvalid_0's l1: 5.09389\n",
      "[2500]\tvalid_0's l1: 5.07473\n",
      "[2600]\tvalid_0's l1: 5.0576\n",
      "[2700]\tvalid_0's l1: 5.04078\n",
      "[2800]\tvalid_0's l1: 5.02349\n",
      "[2900]\tvalid_0's l1: 5.00609\n",
      "[3000]\tvalid_0's l1: 4.98934\n",
      "[3100]\tvalid_0's l1: 4.97379\n",
      "[3200]\tvalid_0's l1: 4.95849\n",
      "[3300]\tvalid_0's l1: 4.94177\n",
      "[3400]\tvalid_0's l1: 4.92666\n",
      "[3500]\tvalid_0's l1: 4.9106\n",
      "[3600]\tvalid_0's l1: 4.89619\n",
      "[3700]\tvalid_0's l1: 4.88075\n",
      "[3800]\tvalid_0's l1: 4.86837\n",
      "[3900]\tvalid_0's l1: 4.85551\n",
      "[4000]\tvalid_0's l1: 4.84186\n",
      "[4100]\tvalid_0's l1: 4.82632\n",
      "[4200]\tvalid_0's l1: 4.81077\n",
      "[4300]\tvalid_0's l1: 4.79652\n",
      "[4400]\tvalid_0's l1: 4.78514\n",
      "[4500]\tvalid_0's l1: 4.77482\n",
      "[4600]\tvalid_0's l1: 4.76435\n",
      "[4700]\tvalid_0's l1: 4.75505\n",
      "[4800]\tvalid_0's l1: 4.74574\n",
      "[4900]\tvalid_0's l1: 4.73646\n",
      "[5000]\tvalid_0's l1: 4.72696\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 4.72696\n",
      "Model for fold 9 saved to modelitos_para_despues/doblez_9.txt\n",
      ":LGB Fold 9 MAE: 4.726960037200103\n",
      "Fold 10 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 4.84257\n",
      "[200]\tvalid_0's l1: 4.82723\n",
      "[300]\tvalid_0's l1: 4.82221\n",
      "[400]\tvalid_0's l1: 4.82232\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's l1: 4.82138\n",
      "Model for fold 10 saved to modelitos_para_despues/doblez_10.txt\n",
      ":LGB Fold 10 MAE: 4.82137608463547\n"
     ]
    }
   ],
   "source": [
    "if LGB:\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 5000,\n",
    "        \"num_leaves\": 512,\n",
    "        \"subsample\": 0.4,\n",
    "        \"colsample_bytree\": 0.6,\n",
    "        \"learning_rate\": 0.00865,\n",
    "        'max_depth': 24,\n",
    "        \"n_jobs\": 4,\n",
    "        \"device\": \"gpu\",\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "        \"reg_alpha\": 0.1,\n",
    "        \"reg_lambda\": 3.25\n",
    "    }\n",
    "\n",
    "    feature_columns = list(df_train_feats.columns)\n",
    "    print(f\"Features = {len(feature_columns)}\")\n",
    "\n",
    "    num_folds = 10\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "\n",
    "    models = []\n",
    "    models_cbt = []\n",
    "    scores = []\n",
    "\n",
    "    model_save_path = 'modelitos_para_despues' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    date_ids = df_train['date_id'].values\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  \n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_columns],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        models.append(lgb_model)\n",
    "        model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "        lgb_model.booster_.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    final_model_params = lgb_params.copy()\n",
    "\n",
    "\n",
    "    num_model = 1\n",
    "\n",
    "    for i in range(num_model):\n",
    "        final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "        final_model.fit(\n",
    "            df_train_feats[feature_columns],\n",
    "            df_train['target'],\n",
    "            callbacks=[\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        models.append(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a27604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE across all folds: 5.666206969630034\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average MAE across all folds\n",
    "average_mae = np.mean(scores)\n",
    "print(f\"Average MAE across all folds: {average_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(num_continuous_features, num_categorical_features, embedding_dims, num_labels, hidden_units, dropout_rates, learning_rate,l2_strength=0.01):\n",
    "\n",
    "    input_continuous = tf.keras.layers.Input(shape=(num_continuous_features,))\n",
    "\n",
    "    input_categorical = [tf.keras.layers.Input(shape=(1,))\n",
    "                         for _ in range(len(num_categorical_features))]\n",
    "\n",
    "    embeddings = [tf.keras.layers.Embedding(input_dim=num_categorical_features[i],\n",
    "                                            output_dim=embedding_dims[i])(input_cat)\n",
    "                  for i, input_cat in enumerate(input_categorical)]\n",
    "    flat_embeddings = [tf.keras.layers.Flatten()(embed) for embed in embeddings]\n",
    "\n",
    "    concat_input = tf.keras.layers.concatenate([input_continuous] + flat_embeddings)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(concat_input)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i],kernel_regularizer=l2(0.01),kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)\n",
    "\n",
    "    out = tf.keras.layers.Dense(num_labels,kernel_regularizer=l2(0.01),kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_continuous] + input_categorical, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_absolute_error',\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
